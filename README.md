# ðŸ§© Agent Basics: LangChain + MCP

Este repo reÃºne ejemplos mÃ­nimos para comprender cÃ³mo LangChain orquesta agentes y cÃ³mo el protocolo MCP (Model Context Protocol) expone herramientas externas que los LLM pueden invocar de forma segura.

---

## ðŸ§  Conceptos express

- **LangChain** ([docs](https://python.langchain.com)): framework para encadenar LLMs con memoria, prompts y herramientas. Sus agentes deciden cuÃ¡ndo llamar funciones externas y cÃ³mo razonar paso a paso.
- **MCP (Model Context Protocol)** ([spec](https://modelcontextprotocol.io/)): estÃ¡ndar abierto para anunciar herramientas (APIs, scripts, DBs) a los modelos. Define cÃ³mo descubrir, describir y ejecutar capacidades vÃ­a transporte `stdio`, WebSockets, etc.
- **Adapters LangChainâ€“MCP**: permiten que un agente de LangChain trate a un servidor MCP como un paquete de `tools`, de modo que pueda ejecutarlos igual que cualquier otra funciÃ³n instrumentada.

---

## ðŸ“‚ QuÃ© hay en este repo

- `mcp_sample/mcp_server_local/server_local.py`: servidor MCP local (FastMCP) con cinco herramientas ðŸ‘‰ suma, resta, multiplicaciÃ³n, divisiÃ³n con validaciÃ³n y un saludo (`hello-mcp`). Ideal para ver cÃ³mo se tipan parÃ¡metros con `typing.Annotated`.
- `mcp_sample/mcp_client/client_async_simple.py`: cliente LangChain asÃ­ncrono que instancia `MultiServerMCPClient`, descubre herramientas `stdio` y crea un agente `ChatOpenAI` con prompt en espaÃ±ol.
- `mcp_sample/mcp_client/client_stdio_simple.py`: variante sÃ­ncrona via `stdio` puro, Ãºtil para depurar.
- `notebooks/`: espacio para experimentos o tutoriales prÃ¡cticos (no incluido en este README, pero recomendado para pruebas).
- `a2a/`: carpeta reservada para exploraciones adicionales (agents-to-agents, integraciones, etc.).

---

## ðŸ“‹ Contenido

- `server_local.py` - Servidor MCP con herramientas matemÃ¡ticas y de saludo
- `client_async_simple.py` - **â­ Cliente AsincrÃ³nico (RECOMENDADO)**
- `client_stdio_simple.py` - Cliente SincrÃ³nico (con conversiÃ³n)

---

## ðŸš€ Inicio RÃ¡pido

### Cliente AsincrÃ³nico (Recomendado)

```bash
# Ejecutar el cliente asincrÃ³nico
cd mcp_sample/mcp_client
conda run -n llm-agent python client_async_simple.py
```

**Salida esperada:**
```
2025-11-14 00:12:10,376 - __main__ - INFO - ðŸš€ Servidor MCP iniciado y corriendo...
2025-11-14 00:12:10,382 - mcp.server.lowlevel.server - INFO - Processing request of type ListToolsRequest
2025-11-14 00:12:24,763 - __main__ - INFO - ðŸš€ Servidor MCP iniciado y corriendo...
2025-11-14 00:12:24,769 - mcp.server.lowlevel.server - INFO - Processing request of type CallToolRequest
2025-11-14 00:12:24,769 - __main__ - INFO - GREET: nombre=Alexander

{
    "messages": [
        "content="" ...
    ]
}
```

## ðŸ”§ Herramientas Disponibles

El servidor MCP expone las siguientes herramientas:

| Herramienta | DescripciÃ³n | ParÃ¡metros |
|-------------|-------------|-----------|
| `add` | Suma dos nÃºmeros | `a: float, b: float` |
| `subtract` | Resta b de a | `a: float, b: float` |
| `multiply` | Multiplica dos valores | `a: float, b: float` |
| `divide` | Divide a entre b | `a: float, b: float` |
| `hello-mcp` | Emite un saludo | `name: str = "amigo"` |

---

## âš™ï¸ CÃ³mo probar

1. **Instala dependencias mÃ­nimas**
   ```bash
   pip install langchain langchain-openai langchain-mcp-adapters fastmcp
   ```
2. **Lanza el servidor MCP local**
   ```bash
   python mcp_sample/mcp_server_local/server_local.py
   ```
3. **Ejecuta el cliente**
   ```bash
   python mcp_sample/mcp_client/client_async_simple.py
   ```
   VerÃ¡s cÃ³mo el agente:
   - Detecta las herramientas publicadas vÃ­a MCP.
   - Saluda al usuario.
   - Llama al sumador remoto para resolver `15 + 27`.

> ðŸ’¡ Usa cualquier endpoint compatible con la API de OpenAI (ej. [`lmstudio.ai`](https://lmstudio.ai), [`docker model runner (DMR)`](https://docs.docker.com/ai/model-runner/api-reference/)) u otros ajustando `base_url`, `model` y `api_key` en el cliente.

---

## ðŸ“š Recursos Ãºtiles

- LangChain Agent Toolkit: https://python.langchain.com/docs/modules/agents/
- MCP reference impl (FastMCP): https://github.com/modelcontextprotocol/fastmcp
- Tutoriales oficiales MCP: https://modelcontextprotocol.io/tutorials
- Ejemplo multi-servidor LangChain + MCP: https://github.com/langchain-ai/langchain-mcp
