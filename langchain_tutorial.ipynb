{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción práctica a LangChain\n",
    "\n",
    "Este cuaderno resume los conceptos esenciales de LangChain con ejemplos listos para ejecutar. Consulta la documentación oficial para ampliar cada tema:\n",
    "- [LangChain Overview](https://docs.langchain.com/oss/python/langchain/overview)\n",
    "- [Integraciones de Proveedores](https://docs.langchain.com/oss/python/integrations/providers/overview)\n",
    "- Referencia adicional: [llamacpp_chat_model_utils.ipynb](https://raw.githubusercontent.com/basaravia/document-parsing-dl-mllm/refs/heads/main/notebooks/llamacpp_chat_model_utils.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "### ¿Qué es LangChain?\n",
    "LangChain es un **kit de orquestación de IA generativa** que uniforma la interacción con modelos, memorias, herramientas y datos. Su objetivo es ayudarte a componer flujos reproducibles que combinen *prompts*, *tool calls* y *state*.\n",
    "\n",
    "### ¿Cuándo usar LangChain, LangGraph, Deep Agents y LangSmith?\n",
    "- `LangChain`: cuando necesitas componer prompts, cadenas o herramientas rápidamente con bloques reutilizables.\n",
    "- `LangGraph`: cuando tu aplicación requiere grafos de control explícitos, ciclos o máquinas de estado para agentes complejos.\n",
    "- `Deep Agents`: cuando priorizas agentes autónomos con razonamiento de largo aliento (planificación, reflexión y delegación intensa).\n",
    "- `LangSmith`: cuando debes **depurar, observar y versionar** tus cadenas/agentes en producción con *tracing* y evaluación continua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación y prerequisitos\n",
    "1. Usa Python ≥3.10 y un entorno virtual limpio.\n",
    "2. Instala `langchain-core` y los conectores que planees usar (OpenAI, AWS, Google, etc.).\n",
    "3. Define tus credenciales mediante variables de entorno para no hardcodear secretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53723c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.9/914.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]3\u001b[0m [ipywidgets]widgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3566a3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain 1.0.4\n",
      "Uninstalling langchain-1.0.4:\n",
      "  Successfully uninstalled langchain-1.0.4\n",
      "Found existing installation: langchain-core 1.0.3\n",
      "Uninstalling langchain-core-1.0.3:\n",
      "  Successfully uninstalled langchain-core-1.0.3\n",
      "Found existing installation: langchain-openai 1.0.2\n",
      "Uninstalling langchain-openai-1.0.2:\n",
      "  Successfully uninstalled langchain-openai-1.0.2\n",
      "Found existing installation: langchain-aws 1.0.0\n",
      "Uninstalling langchain-aws-1.0.0:\n",
      "  Successfully uninstalled langchain-aws-1.0.0\n",
      "Found existing installation: langchain-google-vertexai 3.0.2\n",
      "Uninstalling langchain-google-vertexai-3.0.2:\n",
      "  Successfully uninstalled langchain-google-vertexai-3.0.2\n",
      "Found existing installation: langchain-community 0.4.1\n",
      "Uninstalling langchain-community-0.4.1:\n",
      "  Successfully uninstalled langchain-community-0.4.1\n",
      "Found existing installation: langgraph 1.0.2\n",
      "Uninstalling langgraph-1.0.2:\n",
      "  Successfully uninstalled langgraph-1.0.2\n",
      "Found existing installation: langsmith 0.4.13\n",
      "Uninstalling langsmith-0.4.13:\n",
      "  Successfully uninstalled langsmith-0.4.13\n",
      "\u001b[33mWARNING: Skipping langchain-unstructured as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: faiss-cpu 1.12.0\n",
      "Uninstalling faiss-cpu-1.12.0:\n",
      "  Successfully uninstalled faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y langchain langchain-core langchain-openai langchain-aws langchain-google-vertexai langchain-community langgraph langsmith faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-core langchain langchain-openai langchain-aws langchain-google-vertexai faiss-cpu langchain-community langgraph langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb11b0",
   "metadata": {},
   "source": [
    "### Configuración rápida con proveedores administrados\n",
    "Cada proveedor expone modelos de chat con ligeras variaciones. LangChain ofrece *wrappers* homogéneos para mantener la misma interfaz `invoke`.\n",
    "\n",
    "#### AWS Bedrock (ChatBedrockConverse)\n",
    "Usa el nuevo endpoint *Converse* para acceder a modelos de Anthropic, Cohere o Amazon. Requiere `AWS_REGION`, credenciales IAM y habilitar el modelo en la consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41812933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "bedrock_llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region=os.getenv(\"AWS_REGION\", \"us-east-1\"),\n",
    "    temperature=0.2,\n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "try:\n",
    "    bedrock_reply = bedrock_llm.invoke(\"Resume la misión de LangChain en dos líneas.\")\n",
    "    print(bedrock_reply.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Inicia sesión en AWS y otorga permisos a Bedrock antes de ejecutar. Error: \", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2fb63",
   "metadata": {},
   "source": [
    "#### Azure OpenAI y runtimes compatibles (llama.cpp)\n",
    "Azure ofrece modelos como GPT-4o bajo contrato empresarial y el *OpenAI compatibility layer* permite reutilizar `ChatOpenAI`. El mismo cliente funciona con servidores locales que imiten la API de OpenAI (por ejemplo, `llama.cpp` con `docker run -p 12434:12434 ghcr.io/ggerganov/llama.cpp:server`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74bd86f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[llama.cpp] Soy Gemma, un modelo de lenguaje grande de código abierto creado por Google DeepMind. Estoy disponible públicamente para su uso. Recibo texto e imágenes como entrada y produzco texto como salida.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# azure_llm = ChatOpenAI(\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\", \"dummy\"),\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://<tu-recurso>.openai.azure.com\"),\n",
    "#     azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o-mini\"),\n",
    "#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"),\n",
    "#     temperature=0.2,\n",
    "#     max_tokens=300,\n",
    "# )\n",
    "\n",
    "local_llm = ChatOpenAI(\n",
    "    model=\"ai/gemma3n:latest\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=256,\n",
    "    timeout=300,\n",
    "    max_retries=2,\n",
    "    api_key=\"dummy\",\n",
    "    base_url=\"http://localhost:12434/engines/llama.cpp/v1\",\n",
    ")\n",
    "\n",
    "for name, client in {\"llama.cpp\": local_llm}.items():\n",
    "    try:\n",
    "        reply = client.invoke(\"Explica en español una linea quien eres.\")\n",
    "        print(f\"[{name}] {reply.content}\")\n",
    "    except Exception as exc:  # pragma: no cover\n",
    "        print(f\"[{name}] Configura el endpoint antes de ejecutar. Error: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e11ba",
   "metadata": {},
   "source": [
    "#### Google Vertex AI\n",
    "El SDK `langchain-google-vertexai` expone a Gemini y modelos PaLM. Necesitas autenticarte con `gcloud auth application-default login` o una cuenta de servicio con permisos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c74b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "google_llm = ChatVertexAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=256,\n",
    ")\n",
    "\n",
    "try:\n",
    "    google_reply = google_llm.invoke(\"¿Cómo complementa Vertex AI a LangChain?\")\n",
    "    print(google_reply.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Configura GOOGLE_APPLICATION_CREDENTIALS antes de ejecutar. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1528f3",
   "metadata": {},
   "source": [
    "### LLM `.invoke()` en acción\n",
    "Usamos un `ChatPromptTemplate` para combinar mensajes del sistema y del usuario antes de llamar al modelo. Esta es la base de cualquier cadena en LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6f03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para lanzar un agente privado, necesitas:\n",
      "\n",
      "1. **Un lenguaje de programación:** Python es popular, pero otros como Java o C# también sirven.\n",
      "2. **Un framework de agentes:**  Como ReactiveSwift, JADE, o una implementación personalizada.\n",
      "3. **Un modelo de comportamiento:** Define cómo el agente interactúa con su entorno y otros agentes.\n",
      "4. **Un entorno de simulación/ejecución:**  Un lugar donde el agente pueda operar (puede ser un simulador o un sistema real).\n",
      "5. **Un conjunto de datos/recursos:**  Datos para el aprendizaje, información sobre el mundo, etc.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en {tema}. Responde en español, breve.\"),\n",
    "    (\"human\", \"{pregunta}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | local_llm\n",
    "\n",
    "try:\n",
    "    response = chain.invoke({\n",
    "        \"tema\": \"arquitectura de agentes\",\n",
    "        \"pregunta\": \"¿Qué piezas necesitas para lanzar un agente privado?\",\n",
    "    })\n",
    "    print(response.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Levanta tu servidor llama.cpp (base_url=12434) para probar. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c788da",
   "metadata": {},
   "source": [
    "### Streaming y *callbacks*\n",
    "LangChain soporta *streaming* con `CallbackManager`. Ideal para *demos* o UX reactivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e15abb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker Model Runner es una herramienta de código abierto diseñada para simplificar el despliegue y la ejecución de modelos de aprendizaje automático.  Actúa como un orquestador, encapsulando modelos y sus dependencias en contenedores Docker. Esto garantiza la reproducibilidad y la portabilidad, permitiendo que los modelos se ejecuten de manera consistente en diferentes entornos.\n",
      "\n",
      "Model Runner abstrae la complejidad de la infraestructura subyacente, facilitando la gestión de recursos y el escalado.  Permite la ejecución de modelos con diferentes frameworks (TensorFlow, PyTorch, etc.) y proporciona una interfaz unificada para la monitorización y el registro de métricas.  En esencia, agiliza el ciclo de vida del modelo, desde el desarrollo hasta la producción.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "streaming_llm = ChatOpenAI(\n",
    "    model=\"ai/gemma3n:latest\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1000,\n",
    "    timeout=300,\n",
    "    max_retries=2,\n",
    "    api_key=\"dummy\",\n",
    "    base_url=\"http://localhost:12434/engines/llama.cpp/v1\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "try:\n",
    "    streaming_llm.invoke(\n",
    "        [HumanMessage(content=\"Resume qué es Docker Model Runner en un ensayo de 100 palabras.\")]\n",
    "    )\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Activa el endpoint con streaming antes de ejecutar. Error:\", exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conceptos clave\n",
    "A continuación se describen los bloques fundamentales del ecosistema LangChain. Cada subtema incluye un apunte práctico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos y mensajes\n",
    "Trabaja con objetos `BaseMessage` (`SystemMessage`, `HumanMessage`, `AIMessage`). Esto permite insertar metadatos y mantener historiales consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Como arquitecto de IA, me complace explicar la diferencia entre un LLM (Large Language Model) y un Chat Model. Aunque a menudo se usan indistintamente, existen diferencias sutiles pero importantes en su arquitectura, entrenamiento y propósito.\n",
      "\n",
      "**LLM (Large Language Model) - El Motor de Lenguaje**\n",
      "\n",
      "Imagina un LLM como un motor de lenguaje poderoso. Es un modelo de inteligencia artificial entrenado en una cantidad masiva de datos de texto. Su objetivo principal es **predecir la siguiente palabra en una secuencia**.  Esto se logra a través de redes neuronales profundas, típicamente basadas en la arquitectura Transformer.\n",
      "\n",
      "**Características clave de un LLM:**\n",
      "\n",
      "*   **Entrenamiento masivo:** Se entrenan en terabytes de datos de texto provenientes de internet, libros, artículos, código, etc.\n",
      "*   **Predicción de secuencias:**  Su función principal es predecir la siguiente palabra en una secuencia dada.  Esto les permite generar texto coherente y gramaticalmente correcto.\n",
      "*   **Versatilidad:**  Debido a su entrenamiento masivo, los LLMs son increíblemente versátiles. Pueden ser utilizados para una amplia gama de tareas, incluyendo:\n",
      "    *   Generación de texto (artículos, poemas, código, etc.)\n",
      "    *   Traducción de idiomas\n",
      "    *   Resumen de texto\n",
      "    *   Completar texto\n",
      "    *   Responder preguntas (aunque no siempre de manera óptima)\n",
      "    *   Generación de código\n",
      "*   **Arquitectura Transformer:** La mayoría de los LLMs modernos se basan en la arquitectura Transformer, que es particularmente buena para procesar secuencias de datos como el lenguaje.\n",
      "*   **Ejemplos:** GPT-3, GPT-4, LaMDA, PaLM, Llama 2.\n",
      "\n",
      "**Chat Model - El Aplicador de LLM**\n",
      "\n",
      "Un Chat Model es una **aplicación específica** construida sobre un LLM.  Esencialmente, toma un LLM y lo optimiza para una interacción conversacional.  Piensa en él como un \"carro\" que utiliza el \"motor\" (el LLM).\n",
      "\n",
      "**Características clave de un Chat Model:**\n",
      "\n",
      "*   **Optimizado para diálogo:**  Está diseñado específicamente para mantener conversaciones con usuarios.\n",
      "*   **Entrenamiento adicional (Fine-tuning):**  Además del entrenamiento inicial del LLM, los Chat Models suelen ser \"fine-tuned\" (ajustados) con datos de conversaciones (diálogos, transcripciones de chat, etc.). Esto les ayuda a comprender mejor el contexto conversacional, el tono y las expectativas del usuario.\n",
      "*   **Gestión del contexto:**  Los Chat Models están diseñados para mantener un historial de la conversación (el contexto) para que puedan responder de manera coherente y relevante.\n",
      "*   **Seguridad y control:**  A menudo incorporan mecanismos de seguridad y control para evitar respuestas inapropiadas o dañinas.\n",
      "*   **Ejemplos:** ChatGPT, Bard, Claude.  Estos modelos utilizan LLMs como GPT-3.5 o GPT-4 como su base.\n",
      "\n",
      "**En resumen:**\n",
      "\n",
      "| Característica | LLM (Large Language Model) | Chat Model |\n",
      "|---|---|---|\n",
      "| **Propósito** | Motor de lenguaje general | Aplicación para conversaciones |\n",
      "| **Entrenamiento** | Entrenamiento masivo en texto | Entrenamiento masivo + Fine-tuning con datos de diálogo |\n",
      "| **Versatilidad** | Amplia gama de tareas | Principalmente conversaciones |\n",
      "| **Contexto** | No inherentemente diseñado para contexto | Diseñado para mantener el contexto de la conversación |\n",
      "| **Ejemplos** | GPT-3, LaMDA | ChatGPT, Bard |\n",
      "\n",
      "**Analogía:**\n",
      "\n",
      "Piensa en un LLM como un motor de coche.  Puede ser utilizado en muchos tipos de vehículos.  Un Chat Model es como un coche específico, diseñado para transportar personas de manera eficiente y cómoda.  Utiliza el motor (LLM) pero está optimizado para una tarea particular.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "Un Chat Model es una aplicación construida sobre un LLM.  El LLM proporciona la capacidad de comprender y generar lenguaje, mientras que el Chat Model agrega la funcionalidad necesaria para mantener conversaciones interactivas y coherentes.  La innovación en el campo de la IA está impulsando el desarrollo de LLMs cada vez más potentes y Chat Models cada vez más sofisticados, lo que nos permite crear experiencias de interacción con la IA cada vez más naturales e intuitivas.\n",
      "\n",
      "Espero que esta explicación sea clara.  ¡No dudes en preguntar si tienes alguna otra duda!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "conversation = [\n",
    "    SystemMessage(content=\"Eres un arquitecto de IA.\"),\n",
    "    HumanMessage(content=\"Explica la diferencia entre un LLM y un chat model.\"),\n",
    "]\n",
    "\n",
    "try:\n",
    "    reply = local_llm.invoke(conversation)\n",
    "    print(reply.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Arranca tu backend compatible con OpenAI para probar. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output\n",
    "`with_structured_output` usa validadores (p.ej. Pydantic) para garantizar que el modelo emita un JSON válido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idea='LangGraph' bullet=['**Flexibilidad y Personalización:** LangGraph ofrece un marco flexible para construir flujos de trabajo de LLM personalizados. Permite a los desarrolladores definir y conectar componentes de LLM de manera modular, adaptándose a necesidades específicas.', '**Modularidad y Reutilización:**  La arquitectura modular facilita la reutilización de componentes y la creación de flujos de trabajo complejos a partir de bloques de construcción más pequeños y manejables.', '**Integración con Herramientas Externas:**  LangGraph simplifica la integración de LLMs con diversas herramientas externas, como bases de datos, APIs y otros servicios, ampliando sus capacidades.', '**Depuración y Monitoreo:** Proporciona herramientas para depurar y monitorear flujos de trabajo de LLM, facilitando la identificación y resolución de problemas.', '**Escalabilidad:**  Diseñado para escalar, LangGraph puede manejar flujos de trabajo complejos y grandes volúmenes de datos.', '**Comunidad y Ecosistema:**  Tiene una comunidad activa y un ecosistema en crecimiento, con contribuciones de componentes y herramientas de terceros.', '**Abstracción de la Complejidad:**  Simplifica la complejidad de la interacción con LLMs, proporcionando una capa de abstracción que facilita el desarrollo y el mantenimiento.']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class FeatureSummary(BaseModel):\n",
    "    idea: str = Field(..., description=\"Resumen corto\")\n",
    "    bullet: list[str] = Field(..., description=\"Beneficios clave\")\n",
    "\n",
    "structured_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Devuelve JSON estricto.\"),\n",
    "    (\"human\", \"Resume las ventajas de LangGraph.\"),\n",
    "])\n",
    "\n",
    "structured_chain = structured_prompt | local_llm.with_structured_output(FeatureSummary)\n",
    "\n",
    "try:\n",
    "    structured = structured_chain.invoke({})\n",
    "    print(structured)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Ejecútalo con un endpoint real para verificar el JSON. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middleware (*Runnables* y tuberías)\n",
    "La API de `Runnable` permite insertar pasos de logging, branching o transformaciones sin crear clases nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Middleware] Entradas: {'tema': 'observabilidad', 'pregunta': '¿Cómo ayuda LangSmith al monitoreo?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def tap(inputs):\n",
    "    print(f\"[Middleware] Entradas: {inputs}\")\n",
    "    return inputs\n",
    "\n",
    "debug_chain = tap | prompt | local_llm\n",
    "\n",
    "try:\n",
    "    debug_chain.invoke({\n",
    "        \"tema\": \"observabilidad\",\n",
    "        \"pregunta\": \"¿Cómo ayuda LangSmith al monitoreo?\",\n",
    "    })\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Ejecuta con un LLM activo para ver el pipeline completo. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains\n",
    "Una cadena conecta `Prompt → LLM → Parser`. Puedes combinarlas con operadores (`|`, `.map()`, `.batch()`), o migrar a LangGraph cuando necesites mayor control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En agentes (como IA o sistemas computacionales), la memoria a corto plazo (MCP) es un espacio de almacenamiento temporal que permite retener información relevante para tareas inmediatas.  Es como un \"bloc de notas\" donde se guardan datos recientes para procesarlos rápidamente, antes de que se pierdan.  Su capacidad es limitada y la información se olvida si no se refuerza o se almacena en memoria a largo plazo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "simple_chain = prompt | local_llm | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    result = simple_chain.invoke({\n",
    "        \"tema\": \"memorias\",\n",
    "        \"pregunta\": \"Define memoria a corto plazo en agentes.\"})\n",
    "    print(result)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Conecta un servidor LLM antes de ejecutar. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentes y herramientas\n",
    "Los agentes deciden qué herramienta usar: consultas SQL, navegadores, calculadoras, etc. Reutiliza `tool` o `StructuredTool` para describir entradas/salidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1386249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.4\n"
     ]
    }
   ],
   "source": [
    "import langchain; print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc625d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"¿Cuántas palabras hay en el siguiente texto? 'LangChain facilita la creación de aplicaciones impulsadas por LLMs.'\", additional_kwargs={}, response_metadata={}, id='2824a34c-881d-4f6b-83fe-a16bf599d78f'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 264, 'total_tokens': 319, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-c22473b', 'id': 'chatcmpl-5epDJu8WQhB9d1XyO0xaLpHpgkgpoQsQ', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--c17716ad-fde3-4211-9f9a-01112262b17a-0', tool_calls=[{'name': 'contar_palabras', 'args': {'texto': 'LangChain facilita la creación de aplicaciones impulsadas por LLMs.'}, 'id': 'gqncooJKPXbyj3gsDpqt4ZHUyb37Yc8p', 'type': 'tool_call'}], usage_metadata={'input_tokens': 264, 'output_tokens': 55, 'total_tokens': 319, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='9', name='contar_palabras', id='f761683b-5fb4-4731-9eb4-6b362b611214', tool_call_id='gqncooJKPXbyj3gsDpqt4ZHUyb37Yc8p'), AIMessage(content='Hay 9 palabras en el texto.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 418, 'total_tokens': 436, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-c22473b', 'id': 'chatcmpl-pHwsYh4XknFTwk0HUKZOyUH05OZN9avD', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9f7c5594-e4fe-4c74-be80-29ce3d464351-0', usage_metadata={'input_tokens': 418, 'output_tokens': 18, 'total_tokens': 436, 'input_token_details': {}, 'output_token_details': {}})] \n",
      "\n",
      "Hay 9 palabras en el texto.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "@tool\n",
    "def contar_palabras(texto: str) -> int:\n",
    "    \"\"\"Cuenta palabras en un texto.\"\"\"\n",
    "    return len(texto.split())\n",
    "\n",
    "agent = create_agent(local_llm, tools=[contar_palabras])\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"¿Cuántas palabras hay en el siguiente texto? 'LangChain facilita la creación de aplicaciones impulsadas por LLMs.'\"}]}\n",
    ")\n",
    "\n",
    "print(result[\"messages\"],'\\n')\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-term memory\n",
    "La memoria de corto plazo almacena los últimos turnos para mantener contexto sin saturar el prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba771995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='43aecbfa-4357-42c6-ab8a-62b35ab9e2f6'),\n",
       "  AIMessage(content=\"Hi Bob! It's nice to meet you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 247, 'total_tokens': 268, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-c22473b', 'id': 'chatcmpl-UQcqGpa2m6kVLlyEPcYrfarCIg8BNsSF', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--1d27537e-c7cc-4578-ac74-de5664ec7b83-0', usage_metadata={'input_tokens': 247, 'output_tokens': 21, 'total_tokens': 268, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver  \n",
    "\n",
    "agent2 = create_agent(local_llm, tools=[contar_palabras])\n",
    "\n",
    "\n",
    "agent2.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77b11180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"¿Cuántas palabras hay en el siguiente texto? 'Hola mundo este es mi primer agente.'\", additional_kwargs={}, response_metadata={}, id='bbe0344e-0e94-4c69-841a-8d42c0b363ce'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 259, 'total_tokens': 309, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-c22473b', 'id': 'chatcmpl-lT00Lmc6yCfZMx84C6ACcFk5hrrNtsG1', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--d64233d3-653c-42cb-93d6-dd87815d7ce3-0', tool_calls=[{'name': 'contar_palabras', 'args': {'texto': 'Hola mundo este es mi primer agente.'}, 'id': '4vLUj6sMULAYcpnEK7vomfqfSukorcaD', 'type': 'tool_call'}], usage_metadata={'input_tokens': 259, 'output_tokens': 50, 'total_tokens': 309, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='7', name='contar_palabras', id='523f1078-8f35-4f1c-937e-9a4af6b3c6cf', tool_call_id='4vLUj6sMULAYcpnEK7vomfqfSukorcaD'), AIMessage(content='Hay 7 palabras en el texto.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 400, 'total_tokens': 418, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-c22473b', 'id': 'chatcmpl-5DgvVUxI8MijaP4GpHnvfqftxiB4HI8s', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5d075363-a657-42f2-b96a-9fbe1fbcc182-0', usage_metadata={'input_tokens': 400, 'output_tokens': 18, 'total_tokens': 418, 'input_token_details': {}, 'output_token_details': {}})] \n",
      "\n",
      "Hay 7 palabras en el texto.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "class CustomAgentState(AgentState):  \n",
    "    user_id: str\n",
    "    preferences: dict\n",
    "\n",
    "agent2 = create_agent(\n",
    "    local_llm, \n",
    "    tools=[contar_palabras],\n",
    "    state_schema=CustomAgentState,  \n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "# Custom state can be passed in invoke\n",
    "result = agent2.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"¿Cuántas palabras hay en el siguiente texto? 'Hola mundo este es mi primer agente.'\"}],\n",
    "        \"user_id\": \"user_123\",  \n",
    "        \"preferences\": {\"apodo\": \"agente_IA\"}  \n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "print(result[\"messages\"],'\\n')\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f3052eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Dame información del usuario', additional_kwargs={}, response_metadata={}, id='3c81e4d7-558d-4189-8abf-4e85fa2df526'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 214, 'total_tokens': 240, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-c22473b', 'id': 'chatcmpl-Qdfj43oLZRjEStlM2yvwtJOn5EUwkSnE', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--22aa837b-4532-432a-8cf7-a401f2139ff8-0', tool_calls=[{'name': 'get_user_info', 'args': {}, 'id': 'EJJf9XOO9Ny5D8wnERbachszRltzuJBh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 214, 'output_tokens': 26, 'total_tokens': 240, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='El usuario es John Smith', name='get_user_info', id='744bca82-70af-41df-b50e-af8d4c17d89d', tool_call_id='EJJf9XOO9Ny5D8wnERbachszRltzuJBh'), AIMessage(content='El usuario es John Smith.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 344, 'total_tokens': 360, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-c22473b', 'id': 'chatcmpl-en9xlXexvqJkcs70krCY6yvviIlea4u9', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--c977b423-1602-4755-8c7c-5049a971ca92-0', usage_metadata={'input_tokens': 344, 'output_tokens': 16, 'total_tokens': 360, 'input_token_details': {}, 'output_token_details': {}})] \n",
      "\n",
      "El usuario es John Smith.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_user_info(\n",
    "    runtime: ToolRuntime\n",
    ") -> str:\n",
    "    \"\"\"Look up user info.\"\"\"\n",
    "    user_id = runtime.state[\"user_id\"]\n",
    "    return \"El usuario es John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n",
    "\n",
    "agent3 = create_agent(\n",
    "    local_llm, \n",
    "    tools=[get_user_info],\n",
    "    state_schema=CustomState,\n",
    ")\n",
    "\n",
    "result = agent3.invoke({\n",
    "    \"messages\": \"Dame información del usuario\",\n",
    "    \"user_id\": \"user_123\"\n",
    "})\n",
    "\n",
    "print(result[\"messages\"],'\\n')\n",
    "print(result[\"messages\"][-1].content)\n",
    "# > User is John Smith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conceptos de integraciones\n",
    "LangChain trae adaptadores para fuentes de datos, *embeddings* y almacenes vectoriales. Aquí un vistazo rápido a los más usados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels\n",
    "Puedes mezclar múltiples modelos y seleccionar en tiempo de ejecución según costo, latencia o dominio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_router = {\n",
    "    \"fast\": ChatOpenAI(model=\"ai/qwen3\", base_url=\"http://localhost:12434/engines/llama.cpp/v1\", api_key=\"dummy\"),\n",
    "    \"reliable\": azure_llm,\n",
    "}\n",
    "\n",
    "selection = \"fast\"\n",
    "\n",
    "try:\n",
    "    answer = chat_router[selection].invoke(\"Dame un tip para evaluar prompts.\")\n",
    "    print(answer.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Activa al menos un backend para rutear chats. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitters\n",
    "`RecursiveCharacterTextSplitter` equilibra longitud y solapamiento para RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "raw_text = \"\"\"LangChain facilita pipelines de IA.\n",
    "Permite integrar memorias, herramientas y agentes.\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=40, chunk_overlap=10)\n",
    "chunks = splitter.split_text(raw_text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding models\n",
    "Puedes usar `HuggingFaceEmbeddings`, `OpenAIEmbeddings`, `BedrockEmbeddings`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9771e0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.014573926106095314, 0.03432486206293106, -0.009266335517168045, 0.022159187123179436, -0.028040019795298576]\n",
      "Dimensión del embedding: 1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"ai/mxbai-embed-large\",\n",
    "    api_key=\"dummy\",  \n",
    "    base_url=\"http://localhost:12434/engines/llama.cpp/v1\"\n",
    ")\n",
    "\n",
    "vector = embeddings.embed_query(\"some text to embed\")\n",
    "print(vector[0:5])\n",
    "print(f\"Dimensión del embedding: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured parser\n",
    "`langchain-unstructured` delega la extracción de texto/elementos usando estrategias como `hi_res` para PDFs complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "file_path = \"data/docs/xxxxx.pdf\"\n",
    "\n",
    "loader_local = UnstructuredLoader(\n",
    "    file_path=file_path,\n",
    "    strategy=\"hi_res\",\n",
    "    languages=[\"spa\"],\n",
    ")\n",
    "\n",
    "try:\n",
    "    documents = loader_local.load()\n",
    "    print(f\"Se extrajeron {len(documents)} fragmentos.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Coloca tu PDF en data/docs/xxxxx.pdf o actualiza la ruta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders\n",
    "Los `DocumentLoader` normalizan fuentes heterogéneas: archivos, sitios web, bases de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "\n",
    "# PDF\n",
    "pdf_loader = PyPDFLoader(\"data/docs/ejemplo.pdf\")\n",
    "\n",
    "# Web\n",
    "web_loader = WebBaseLoader(\"https://docs.langchain.com/oss/python/langchain/overview\")\n",
    "\n",
    "print(\"Loaders listos. Ejecuta load() cuando quieras materializar los documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores (FAISS local)\n",
    "FAISS es un índice vectorial rápido para búsquedas semánticas en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112ee2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de embeddings 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Assuming you have your documents and embeddings initialized\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"ai/granite-embedding-multilingual:latest\",\n",
    "    api_key=\"dummy\",  \n",
    "    base_url=\"http://localhost:12434/engines/llama.cpp/v1\"\n",
    ")\n",
    "\n",
    "print(f'Dimension de embeddings {len(embeddings.embed_query(\"hello world\"))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15514edc",
   "metadata": {},
   "source": [
    "https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2557f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "index = faiss.IndexFlatL2(768)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore= InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "vector_store.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15e238a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added successfully\n"
     ]
    }
   ],
   "source": [
    "# Create documents with valid text content\n",
    "document_1 = Document(page_content=\"LangChain es una herramienta de orquestación\", metadata={\"tipo\": \"descripcion\"})\n",
    "document_2 = Document(page_content=\"Los agentes pueden usar múltiples herramientas\", metadata={\"tipo\": \"funcionalidad\"})\n",
    "document_3 = Document(page_content=\"FAISS permite búsquedas vectoriales eficientes\", metadata={\"tipo\": \"componente\"})\n",
    "\n",
    "documents = [document_1, document_2, document_3]\n",
    "ids = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "\n",
    "try:\n",
    "\tvector_store.add_documents(documents=documents, ids=ids)\n",
    "\tprint(\"Documents added successfully\")\n",
    "except Exception as e:\n",
    "\tprint(f\"Error adding documents: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff9d89e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def clean(text):\n",
    "    import re\n",
    "    text = unidecode(text)  # Convierte tildes a ASCII\n",
    "    return re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "\n",
    "# Aplica clean al query\n",
    "query = \"¿Quién controla el flujo de un agente complejo?\"\n",
    "\n",
    "results = vector_store.similarity_search(query=query, k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector_store = FAISS.load_local(\n",
    "\"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Próximos pasos\n",
    "1. Conecta LangSmith para trazar y depurar tus cadenas en producción.\n",
    "2. Lleva tus agentes a LangGraph cuando necesiten ciclos o ramificaciones complejas.\n",
    "3. Experimenta con `RunnableWithRetry`, `ToolNode` y `StateGraph` para robustecer tus agentes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
