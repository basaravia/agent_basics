{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción práctica a LangChain\n",
    "\n",
    "Este cuaderno resume los conceptos esenciales de LangChain con ejemplos listos para ejecutar. Consulta la documentación oficial para ampliar cada tema:\n",
    "- [LangChain Overview](https://docs.langchain.com/oss/python/langchain/overview)\n",
    "- [Integraciones de Proveedores](https://docs.langchain.com/oss/python/integrations/providers/overview)\n",
    "- Referencia adicional: [llamacpp_chat_model_utils.ipynb](https://raw.githubusercontent.com/basaravia/document-parsing-dl-mllm/refs/heads/main/notebooks/llamacpp_chat_model_utils.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "### ¿Qué es LangChain?\n",
    "LangChain es un **kit de orquestación de IA generativa** que uniforma la interacción con modelos, memorias, herramientas y datos. Su objetivo es ayudarte a componer flujos reproducibles que combinen *prompts*, *tool calls* y *state*.\n",
    "\n",
    "### ¿Cuándo usar LangChain, LangGraph, Deep Agents y LangSmith?\n",
    "- `LangChain`: cuando necesitas componer prompts, cadenas o herramientas rápidamente con bloques reutilizables.\n",
    "- `LangGraph`: cuando tu aplicación requiere grafos de control explícitos, ciclos o máquinas de estado para agentes complejos.\n",
    "- `Deep Agents`: cuando priorizas agentes autónomos con razonamiento de largo aliento (planificación, reflexión y delegación intensa).\n",
    "- `LangSmith`: cuando debes **depurar, observar y versionar** tus cadenas/agentes en producción con *tracing* y evaluación continua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación y prerequisitos\n",
    "1. Usa Python ≥3.10 y un entorno virtual limpio.\n",
    "2. Instala `langchain-core` y los conectores que planees usar (OpenAI, AWS, Google, etc.).\n",
    "3. Define tus credenciales mediante variables de entorno para no hardcodear secretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53723c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.9/914.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]3\u001b[0m [ipywidgets]widgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3566a3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain 1.0.4\n",
      "Uninstalling langchain-1.0.4:\n",
      "  Successfully uninstalled langchain-1.0.4\n",
      "Found existing installation: langchain-core 1.0.3\n",
      "Uninstalling langchain-core-1.0.3:\n",
      "  Successfully uninstalled langchain-core-1.0.3\n",
      "Found existing installation: langchain-openai 1.0.2\n",
      "Uninstalling langchain-openai-1.0.2:\n",
      "  Successfully uninstalled langchain-openai-1.0.2\n",
      "Found existing installation: langchain-aws 1.0.0\n",
      "Uninstalling langchain-aws-1.0.0:\n",
      "  Successfully uninstalled langchain-aws-1.0.0\n",
      "Found existing installation: langchain-google-vertexai 3.0.2\n",
      "Uninstalling langchain-google-vertexai-3.0.2:\n",
      "  Successfully uninstalled langchain-google-vertexai-3.0.2\n",
      "Found existing installation: langchain-community 0.4.1\n",
      "Uninstalling langchain-community-0.4.1:\n",
      "  Successfully uninstalled langchain-community-0.4.1\n",
      "Found existing installation: langgraph 1.0.2\n",
      "Uninstalling langgraph-1.0.2:\n",
      "  Successfully uninstalled langgraph-1.0.2\n",
      "Found existing installation: langsmith 0.4.13\n",
      "Uninstalling langsmith-0.4.13:\n",
      "  Successfully uninstalled langsmith-0.4.13\n",
      "\u001b[33mWARNING: Skipping langchain-unstructured as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: faiss-cpu 1.12.0\n",
      "Uninstalling faiss-cpu-1.12.0:\n",
      "  Successfully uninstalled faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y langchain langchain-core langchain-openai langchain-aws langchain-google-vertexai langchain-community langgraph langsmith faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-core langchain langchain-openai langchain-aws langchain-google-vertexai faiss-cpu langchain-community langgraph langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb11b0",
   "metadata": {},
   "source": [
    "### Configuración rápida con proveedores administrados\n",
    "Cada proveedor expone modelos de chat con ligeras variaciones. LangChain ofrece *wrappers* homogéneos para mantener la misma interfaz `invoke`.\n",
    "\n",
    "#### AWS Bedrock (ChatBedrockConverse)\n",
    "Usa el nuevo endpoint *Converse* para acceder a modelos de Anthropic, Cohere o Amazon. Requiere `AWS_REGION`, credenciales IAM y habilitar el modelo en la consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41812933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/basaravia/miniconda3/envs/llm-agent/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3639: UserWarning: WARNING! region is not default parameter.\n",
      "                region was transferred to model_kwargs.\n",
      "                Please confirm that region is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicia sesión en AWS y otorga permisos a Bedrock antes de ejecutar. Error:  An error occurred (UnrecognizedClientException) when calling the Converse operation: The security token included in the request is invalid.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "bedrock_llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    region=os.getenv(\"AWS_REGION\", \"us-east-1\"),\n",
    "    temperature=0.2,\n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "try:\n",
    "    bedrock_reply = bedrock_llm.invoke(\"Resume la misión de LangChain en dos líneas.\")\n",
    "    print(bedrock_reply.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Inicia sesión en AWS y otorga permisos a Bedrock antes de ejecutar. Error: \", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2fb63",
   "metadata": {},
   "source": [
    "#### Azure OpenAI y runtimes compatibles (llama.cpp)\n",
    "Azure ofrece modelos como GPT-4o bajo contrato empresarial y el *OpenAI compatibility layer* permite reutilizar `ChatOpenAI`. El mismo cliente funciona con servidores locales que imiten la API de OpenAI (por ejemplo, `llama.cpp` con `docker run -p 12434:12434 ghcr.io/ggerganov/llama.cpp:server`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74bd86f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[llama.cpp] Soy Gemma, un modelo de lenguaje grande de código abierto creado por Google DeepMind. Estoy disponible públicamente para que todos puedan usarme. Recibo texto e imágenes como entradas y produzco texto como salida.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# azure_llm = ChatOpenAI(\n",
    "#     api_key=os.getenv(\"AZURE_OPENAI_API_KEY\", \"dummy\"),\n",
    "#     azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://<tu-recurso>.openai.azure.com\"),\n",
    "#     azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o-mini\"),\n",
    "#     api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"),\n",
    "#     temperature=0.2,\n",
    "#     max_tokens=300,\n",
    "# )\n",
    "\n",
    "local_llm = ChatOpenAI(\n",
    "    model=\"ai/gemma3n:latest\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=256,\n",
    "    timeout=300,\n",
    "    max_retries=2,\n",
    "    api_key=\"dummy\",\n",
    "    base_url=\"http://localhost:12434/engines/llama.cpp/v1\",\n",
    ")\n",
    "\n",
    "for name, client in {\"llama.cpp\": local_llm}.items():\n",
    "    try:\n",
    "        reply = client.invoke(\"Explica en español una linea quien eres.\")\n",
    "        print(f\"[{name}] {reply.content}\")\n",
    "    except Exception as exc:  # pragma: no cover\n",
    "        print(f\"[{name}] Configura el endpoint antes de ejecutar. Error: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e11ba",
   "metadata": {},
   "source": [
    "#### Google Vertex AI\n",
    "El SDK `langchain-google-vertexai` expone a Gemini y modelos PaLM. Necesitas autenticarte con `gcloud auth application-default login` o una cuenta de servicio con permisos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c74b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "google_llm = ChatVertexAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=256,\n",
    ")\n",
    "\n",
    "try:\n",
    "    google_reply = google_llm.invoke(\"¿Cómo complementa Vertex AI a LangChain?\")\n",
    "    print(google_reply.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Configura GOOGLE_APPLICATION_CREDENTIALS antes de ejecutar. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1528f3",
   "metadata": {},
   "source": [
    "### LLM `.invoke()` en acción\n",
    "Usamos un `ChatPromptTemplate` para combinar mensajes del sistema y del usuario antes de llamar al modelo. Esta es la base de cualquier cadena en LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6f03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para lanzar un agente privado, necesitas:\n",
      "\n",
      "1. **Un lenguaje de programación:** Python es popular, pero otros como Java o C# también sirven.\n",
      "2. **Un framework de agentes:**  Como ReactiveSwift, JADE, o una implementación personalizada.\n",
      "3. **Un modelo de comportamiento:** Define cómo el agente interactúa con su entorno y otros agentes.\n",
      "4. **Un entorno de simulación/ejecución:**  Un lugar donde el agente pueda operar (puede ser un simulador o un sistema real).\n",
      "5. **Un conjunto de datos/recursos:**  Datos para el aprendizaje, información sobre el mundo, etc.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en {tema}. Responde en español, breve.\"),\n",
    "    (\"human\", \"{pregunta}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | local_llm\n",
    "\n",
    "try:\n",
    "    response = chain.invoke({\n",
    "        \"tema\": \"arquitectura de agentes\",\n",
    "        \"pregunta\": \"¿Qué piezas necesitas para lanzar un agente privado?\",\n",
    "    })\n",
    "    print(response.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Levanta tu servidor llama.cpp (base_url=12434) para probar. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c788da",
   "metadata": {},
   "source": [
    "### Streaming y *callbacks*\n",
    "LangChain soporta *streaming* con `CallbackManager`. Ideal para *demos* o UX reactivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e15abb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker Model Runner es una herramienta de código abierto diseñada para simplificar el despliegue y la ejecución de modelos de aprendizaje automático.  Funciona encapsulando modelos y sus dependencias en contenedores Docker, asegurando la reproducibilidad y la portabilidad en diferentes entornos. \n",
      "\n",
      "Simplifica la creación de pipelines de inferencia, permitiendo a los usuarios desplegar modelos con facilidad, gestionar versiones y monitorizar su rendimiento.  Al abstraer la complejidad de la infraestructura subyacente, Model Runner permite a los equipos de ML centrarse en el desarrollo de modelos, en lugar de en la gestión de la infraestructura.  Es ideal para desplegar modelos en la nube, en el borde o en entornos locales.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "streaming_llm = ChatOpenAI(\n",
    "    model=\"ai/gemma3n:latest\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1000,\n",
    "    timeout=300,\n",
    "    max_retries=2,\n",
    "    api_key=\"dummy\",\n",
    "    base_url=\"http://localhost:12434/engines/llama.cpp/v1\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "try:\n",
    "    streaming_llm.invoke(\n",
    "        [HumanMessage(content=\"Resume qué es Docker Model Runner en un ensayo de 100 palabras.\")]\n",
    "    )\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Activa el endpoint con streaming antes de ejecutar. Error:\", exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conceptos clave\n",
    "A continuación se describen los bloques fundamentales del ecosistema LangChain. Cada subtema incluye un apunte práctico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos y mensajes\n",
    "Trabaja con objetos `BaseMessage` (`SystemMessage`, `HumanMessage`, `AIMessage`). Esto permite insertar metadatos y mantener historiales consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Excelente pregunta! Como arquitecto de IA, me complace explicar la diferencia entre un LLM (Large Language Model) y un Chat Model. Aunque a menudo se usan indistintamente, hay una distinción importante en su arquitectura y propósito.\n",
      "\n",
      "**LLM (Large Language Model) - El Motor Fundamental**\n",
      "\n",
      "Imagina un LLM como el **motor principal** que impulsa la generación de texto.  Es un modelo de aprendizaje automático entrenado en una cantidad masiva de datos de texto.  Estos datos pueden incluir libros, artículos, código, sitios web, y mucho más.  \n",
      "\n",
      "**Características clave de un LLM:**\n",
      "\n",
      "*   **Generación de texto:** Su principal función es predecir la siguiente palabra en una secuencia, basándose en el contexto previo.  Esto le permite generar texto coherente y gramaticalmente correcto.\n",
      "*   **Amplio conocimiento:** Debido a la gran cantidad de datos con los que se entrena, los LLM tienen un amplio conocimiento sobre una variedad de temas.\n",
      "*   **Versatilidad:**  Pueden realizar una amplia gama de tareas de procesamiento del lenguaje natural (PNL), como:\n",
      "    *   Traducción de idiomas\n",
      "    *   Resumen de texto\n",
      "    *   Generación de código\n",
      "    *   Completar texto\n",
      "    *   Responder preguntas (de forma general)\n",
      "    *   Análisis de sentimientos\n",
      "*   **Arquitectura:**  La mayoría de los LLM modernos se basan en la arquitectura Transformer.  Esta arquitectura es particularmente buena para capturar relaciones a largo alcance en el texto.\n",
      "*   **Ejemplos:** GPT-3, GPT-4, LaMDA, PaLM, Llama 2.\n",
      "\n",
      "**Chat Model - La Aplicación Específica**\n",
      "\n",
      "Un Chat Model es una **aplicación construida sobre un LLM**.  Esencialmente, toma un LLM y lo optimiza para una interacción conversacional.  Piensa en él como un \"carro\" que utiliza el \"motor\" (el LLM).\n",
      "\n",
      "**Características clave de un Chat Model:**\n",
      "\n",
      "*   **Diseñado para la conversación:**  Está específicamente diseñado para mantener conversaciones coherentes y atractivas con los usuarios.\n",
      "*   **Entrenamiento adicional:**  Además del entrenamiento inicial del LLM, los Chat Models suelen ser entrenados con datos de conversaciones (diálogos) para mejorar su capacidad de responder preguntas, mantener el contexto y generar respuestas apropiadas.\n",
      "*   **Gestión del contexto:**  Los Chat Models están diseñados para recordar el historial de la conversación (el contexto) para proporcionar respuestas más relevantes.\n",
      "*   **Seguridad y moderación:**  A menudo incorporan mecanismos de seguridad y moderación para evitar respuestas inapropiadas o dañinas.\n",
      "*   **Ejemplos:** ChatGPT, Bard, Claude.\n",
      "\n",
      "**En resumen, la diferencia clave:**\n",
      "\n",
      "| Característica | LLM (Large Language Model) | Chat Model |\n",
      "|---|---|---|\n",
      "| **Función principal** | Generación de texto general | Interacción conversacional |\n",
      "| **Alcance** | Amplio, puede realizar muchas tareas de PNL | Específico, optimizado para el diálogo |\n",
      "| **Entrenamiento** | Entrenado en grandes cantidades de texto | Entrenado en texto + datos de conversación |\n",
      "| **Ejemplo** | GPT-4 | ChatGPT |\n",
      "\n",
      "**Analogía:**\n",
      "\n",
      "Piensa en un LLM como un motor de coche.  Puede proporcionar la potencia para mover el coche.  Un Chat Model es el coche completo, con el motor (LLM), el chasis, el volante, etc., diseñados para conducir y transportar personas.\n",
      "\n",
      "**¿Por qué esta distinción es importante?**\n",
      "\n",
      "*   **Flexibilidad:**  Los LLM pueden usarse para muchas cosas diferentes, mientras que los Chat Models están optimizados para una tarea específica.\n",
      "*   **Personalización:**  Puedes usar un LLM como base y luego construir un Chat Model personalizado para una aplicación específica.\n",
      "*   **Eficiencia:**  Construir un Chat Model a partir de un LLM existente puede ser más eficiente que entrenar un modelo desde cero.\n",
      "\n",
      "Espero que esta explicación aclare la diferencia entre un LLM y un Chat Model.  ¡Avísame si tienes alguna otra pregunta!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "conversation = [\n",
    "    SystemMessage(content=\"Eres un arquitecto de IA.\"),\n",
    "    HumanMessage(content=\"Explica la diferencia entre un LLM y un chat model.\"),\n",
    "]\n",
    "\n",
    "try:\n",
    "    reply = local_llm.invoke(conversation)\n",
    "    print(reply.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Arranca tu backend compatible con OpenAI para probar. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output\n",
    "`with_structured_output` usa validadores (p.ej. Pydantic) para garantizar que el modelo emita un JSON válido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idea='LangGraph' bullet=['**Flexibilidad y Personalización:** LangGraph ofrece un marco flexible para construir flujos de trabajo de LLM personalizados. Permite a los desarrolladores definir y conectar componentes de LLM de manera modular, adaptándose a necesidades específicas.', '**Modularidad y Reutilización:**  La arquitectura modular facilita la reutilización de componentes y la creación de flujos de trabajo complejos a partir de bloques de construcción más pequeños y manejables.', '**Integración con Herramientas Externas:**  LangGraph simplifica la integración de LLMs con diversas herramientas externas, como bases de datos, APIs y otros servicios, ampliando sus capacidades.', '**Depuración y Monitoreo:** Proporciona herramientas para depurar y monitorear flujos de trabajo de LLM, facilitando la identificación y resolución de problemas.', '**Escalabilidad:**  Diseñado para escalar, LangGraph puede manejar flujos de trabajo complejos y grandes volúmenes de datos.', '**Comunidad y Ecosistema:**  Tiene una comunidad activa y un ecosistema en crecimiento, con contribuciones de componentes y herramientas de terceros.', '**Abstracción de la Complejidad:**  Simplifica la complejidad de la interacción con LLMs, proporcionando una capa de abstracción que facilita el desarrollo y el mantenimiento.']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class FeatureSummary(BaseModel):\n",
    "    idea: str = Field(..., description=\"Resumen corto\")\n",
    "    bullet: list[str] = Field(..., description=\"Beneficios clave\")\n",
    "\n",
    "structured_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Devuelve JSON estricto.\"),\n",
    "    (\"human\", \"Resume las ventajas de LangGraph.\"),\n",
    "])\n",
    "\n",
    "structured_chain = structured_prompt | local_llm.with_structured_output(FeatureSummary)\n",
    "\n",
    "try:\n",
    "    structured = structured_chain.invoke({})\n",
    "    print(structured)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Ejecútalo con un endpoint real para verificar el JSON. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middleware (*Runnables* y tuberías)\n",
    "La API de `Runnable` permite insertar pasos de logging, branching o transformaciones sin crear clases nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Middleware] Entradas: {'tema': 'observabilidad', 'pregunta': '¿Cómo ayuda LangSmith al monitoreo?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def tap(inputs):\n",
    "    print(f\"[Middleware] Entradas: {inputs}\")\n",
    "    return inputs\n",
    "\n",
    "debug_chain = tap | prompt | local_llm\n",
    "\n",
    "try:\n",
    "    debug_chain.invoke({\n",
    "        \"tema\": \"observabilidad\",\n",
    "        \"pregunta\": \"¿Cómo ayuda LangSmith al monitoreo?\",\n",
    "    })\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Ejecuta con un LLM activo para ver el pipeline completo. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains\n",
    "Una cadena conecta `Prompt → LLM → Parser`. Puedes combinarlas con operadores (`|`, `.map()`, `.batch()`), o migrar a LangGraph cuando necesites mayor control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En agentes (como IA o sistemas computacionales), la memoria a corto plazo (MCP) es un espacio de almacenamiento temporal que permite retener información relevante para tareas inmediatas.  Es como un \"bloc de notas\" donde se guardan datos recientes para procesarlos rápidamente, antes de que se pierdan.  Su capacidad es limitada y la información se olvida si no se refuerza o se almacena en memoria a largo plazo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "simple_chain = prompt | local_llm | StrOutputParser()\n",
    "\n",
    "try:\n",
    "    result = simple_chain.invoke({\n",
    "        \"tema\": \"memorias\",\n",
    "        \"pregunta\": \"Define memoria a corto plazo en agentes.\"})\n",
    "    print(result)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Conecta un servidor LLM antes de ejecutar. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentes y herramientas\n",
    "Los agentes deciden qué herramienta usar: consultas SQL, navegadores, calculadoras, etc. Reutiliza `tool` o `StructuredTool` para describir entradas/salidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1386249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.4\n"
     ]
    }
   ],
   "source": [
    "import langchain; print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc625d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Me llamo Alexander, saludame por favor. y cuenta cuantas palabras hay en este mensaje', additional_kwargs={}, response_metadata={}, id='300f2d8a-7d32-466c-a5a7-63ea81e6e1dc'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 367, 'total_tokens': 408, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-97d5117', 'id': 'chatcmpl-aVQwlgSr1xrfDfslHKEOGRu4aCD4fHNW', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--95464537-2e68-4339-b353-5c0706f626a6-0', tool_calls=[{'name': 'saluda', 'args': {'nombre': 'Alexander'}, 'id': '5XbUxC3dwL01UaOkukfOO7p23QgoTwBp', 'type': 'tool_call'}], usage_metadata={'input_tokens': 367, 'output_tokens': 41, 'total_tokens': 408, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='Hola como estas Alexander: Use la tool determinada', name='saluda', id='74a5ebce-5405-4c38-ab67-e75b2f1dd46f', tool_call_id='5XbUxC3dwL01UaOkukfOO7p23QgoTwBp'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 519, 'total_tokens': 571, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-97d5117', 'id': 'chatcmpl-7ii9SbZVdXEnFrkG5urQBdoRERkVFrvB', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--0323f361-1b01-4c68-870f-deb62002ea6c-0', tool_calls=[{'name': 'contar_palabras', 'args': {'texto': 'Hola como estas Alexander: Use la tool determinada'}, 'id': 'VQ63dZXzuh8m42C1Q9pVKIuQdeTW3cuq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 519, 'output_tokens': 52, 'total_tokens': 571, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='8', name='contar_palabras', id='f0f90064-3d0a-4908-93dc-ef319695e564', tool_call_id='VQ63dZXzuh8m42C1Q9pVKIuQdeTW3cuq'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 674, 'total_tokens': 715, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-97d5117', 'id': 'chatcmpl-E6pqEAsOiT5dXyIfUpncKefqQFlLWSx1', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--2802c830-a743-411e-9e9a-c0a9f739946e-0', tool_calls=[{'name': 'saluda', 'args': {'nombre': 'Alexander'}, 'id': '1svtAKnooVK2dGxeIS8Uyq6cv1sFzOR9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 674, 'output_tokens': 41, 'total_tokens': 715, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='Hola como estas Alexander: Use la tool determinada', name='saluda', id='aa9e4e3d-6eeb-45ab-9cbe-44a660057d71', tool_call_id='1svtAKnooVK2dGxeIS8Uyq6cv1sFzOR9'), AIMessage(content='¡Hola! Me alegra saber que estás bien.  He contado 8 palabras en tu mensaje anterior.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 824, 'total_tokens': 853, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-97d5117', 'id': 'chatcmpl-A5jOxqWIUqfPfIaRqHL5rN5Xa3SO3s19', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--a209a911-b326-4220-987e-814c47294f99-0', usage_metadata={'input_tokens': 824, 'output_tokens': 29, 'total_tokens': 853, 'input_token_details': {}, 'output_token_details': {}})] \n",
      "\n",
      "¡Hola! Me alegra saber que estás bien.  He contado 8 palabras en tu mensaje anterior.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "\n",
    "@tool\n",
    "def contar_palabras(texto: str) -> int:\n",
    "    \"\"\"Cuenta palabras en un texto.\"\"\"\n",
    "    return len(texto.split())\n",
    "\n",
    "@tool\n",
    "def saluda(nombre: str) -> str:\n",
    "    \"\"\"Saluda al usuario en funcion de la entrada de su nombre.\"\"\"\n",
    "    return f\"Hola como estas {nombre}: Use la tool determinada\"\n",
    "\n",
    "agent = create_agent(local_llm, tools=[contar_palabras, saluda])\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Me llamo Alexander, saludame por favor. y cuenta cuantas palabras hay en este mensaje\"}]}\n",
    ")\n",
    "\n",
    "print(result[\"messages\"],'\\n')\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-term memory\n",
    "La memoria de corto plazo almacena los últimos turnos para mantener contexto sin saturar el prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba771995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='0d8d139c-1fb6-4124-a9c0-54afebac2908'),\n",
       "  AIMessage(content=\"Hi Bob! It's nice to meet you.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 247, 'total_tokens': 268, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-97d5117', 'id': 'chatcmpl-1GhG8lZcVgQf0gmPPGfgZlf0rGeh6dxk', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--4966f657-f5fb-41cb-92fd-91cb28489994-0', usage_metadata={'input_tokens': 247, 'output_tokens': 21, 'total_tokens': 268, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver  \n",
    "\n",
    "agent2 = create_agent(local_llm, tools=[contar_palabras])\n",
    "\n",
    "\n",
    "agent2.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77b11180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"¿Cuántas palabras hay en el siguiente texto? 'Hola mundo este es mi primer agente.'\", additional_kwargs={}, response_metadata={}, id='724370ce-49ac-4932-8501-7a8000d5de38'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 259, 'total_tokens': 309, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-97d5117', 'id': 'chatcmpl-LWL2OBOeGo7Y1mzipBBNQ9nmwTGSLhIF', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--ddb48172-698a-468c-bd22-0fca68d31df9-0', tool_calls=[{'name': 'contar_palabras', 'args': {'texto': 'Hola mundo este es mi primer agente.'}, 'id': '8HFIectbhS750S1LbTFXQc6PrLBwtgjS', 'type': 'tool_call'}], usage_metadata={'input_tokens': 259, 'output_tokens': 50, 'total_tokens': 309, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='7', name='contar_palabras', id='b3e23218-6917-4423-98fe-8db777a73347', tool_call_id='8HFIectbhS750S1LbTFXQc6PrLBwtgjS'), AIMessage(content='Hay 7 palabras en el texto.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 408, 'total_tokens': 426, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-97d5117', 'id': 'chatcmpl-bn52XOCPndgOj3qrHfbKyeIUwxsJayOB', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--bbcc96d9-9324-4a82-b693-8d62eac82726-0', usage_metadata={'input_tokens': 408, 'output_tokens': 18, 'total_tokens': 426, 'input_token_details': {}, 'output_token_details': {}})] \n",
      "\n",
      "Hay 7 palabras en el texto.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "class CustomAgentState(AgentState):  \n",
    "    user_id: str\n",
    "    preferences: dict\n",
    "\n",
    "agent2 = create_agent(\n",
    "    local_llm, \n",
    "    tools=[contar_palabras],\n",
    "    state_schema=CustomAgentState,  \n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "# Custom state can be passed in invoke\n",
    "result = agent2.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"¿Cuántas palabras hay en el siguiente texto? 'Hola mundo este es mi primer agente.'\"}],\n",
    "        \"user_id\": \"user_123\",  \n",
    "        \"preferences\": {\"apodo\": \"agente_IA\"}  \n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "print(result[\"messages\"],'\\n')\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3052eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Dame información del usuario', additional_kwargs={}, response_metadata={}, id='b2d5537a-e621-49d6-ba21-6fa967747384'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 214, 'total_tokens': 240, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-97d5117', 'id': 'chatcmpl-WeCSjqaETHTVzmLZsxOpjq3JrJq3ZCf4', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--c2018433-cc8b-4b18-8105-04f6b8e1b9d0-0', tool_calls=[{'name': 'get_user_info', 'args': {}, 'id': 'IaAEqinoyRFUmhkHrAU2PKeNRRN6crUU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 214, 'output_tokens': 26, 'total_tokens': 240, 'input_token_details': {}, 'output_token_details': {}}), ToolMessage(content='El usuario es John Smith', name='get_user_info', id='d9275fa8-8168-4660-a673-29508fead5c3', tool_call_id='IaAEqinoyRFUmhkHrAU2PKeNRRN6crUU'), AIMessage(content='El usuario es John Smith.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 340, 'total_tokens': 356, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'ai/gemma3n:latest', 'system_fingerprint': 'b1-97d5117', 'id': 'chatcmpl-HVDi5471mAPAwXgQqdQDSIOy1MUT3PJC', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--157ae15a-8b69-468a-881c-888039d164fd-0', usage_metadata={'input_tokens': 340, 'output_tokens': 16, 'total_tokens': 356, 'input_token_details': {}, 'output_token_details': {}})] \n",
      "\n",
      "El usuario es John Smith.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def get_user_info(\n",
    "    runtime: ToolRuntime\n",
    ") -> str:\n",
    "    \"\"\"Look up user info.\"\"\"\n",
    "    user_id = runtime.state[\"user_id\"]\n",
    "    return \"El usuario es John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n",
    "\n",
    "agent3 = create_agent(\n",
    "    local_llm, \n",
    "    tools=[get_user_info],\n",
    "    state_schema=CustomState,\n",
    ")\n",
    "\n",
    "result = agent3.invoke({\n",
    "    \"messages\": \"Dame información del usuario\",\n",
    "    \"user_id\": \"user_123\"\n",
    "})\n",
    "\n",
    "print(result[\"messages\"],'\\n')\n",
    "print(result[\"messages\"][-1].content)\n",
    "# > User is John Smith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conceptos de integraciones\n",
    "LangChain trae adaptadores para fuentes de datos, *embeddings* y almacenes vectoriales. Aquí un vistazo rápido a los más usados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatModels\n",
    "Puedes mezclar múltiples modelos y seleccionar en tiempo de ejecución según costo, latencia o dominio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_router = {\n",
    "    \"fast\": ChatOpenAI(model=\"ai/qwen3\", base_url=\"http://localhost:12434/engines/llama.cpp/v1\", api_key=\"dummy\"),\n",
    "    \"reliable\": azure_llm,\n",
    "}\n",
    "\n",
    "selection = \"fast\"\n",
    "\n",
    "try:\n",
    "    answer = chat_router[selection].invoke(\"Dame un tip para evaluar prompts.\")\n",
    "    print(answer.content)\n",
    "except Exception as exc:  # pragma: no cover\n",
    "    print(\"Activa al menos un backend para rutear chats. Error:\", exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitters\n",
    "`RecursiveCharacterTextSplitter` equilibra longitud y solapamiento para RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "raw_text = \"\"\"LangChain facilita pipelines de IA.\n",
    "Permite integrar memorias, herramientas y agentes.\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=40, chunk_overlap=10)\n",
    "chunks = splitter.split_text(raw_text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding models\n",
    "Puedes usar `HuggingFaceEmbeddings`, `OpenAIEmbeddings`, `BedrockEmbeddings`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9771e0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.014573926106095314, 0.03432486206293106, -0.009266335517168045, 0.022159187123179436, -0.028040019795298576]\n",
      "Dimensión del embedding: 1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"ai/mxbai-embed-large\",\n",
    "    api_key=\"dummy\",  \n",
    "    base_url=\"http://localhost:12434/engines/llama.cpp/v1\"\n",
    ")\n",
    "\n",
    "vector = embeddings.embed_query(\"some text to embed\")\n",
    "print(vector[0:5])\n",
    "print(f\"Dimensión del embedding: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured parser\n",
    "`langchain-unstructured` delega la extracción de texto/elementos usando estrategias como `hi_res` para PDFs complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "file_path = \"data/docs/xxxxx.pdf\"\n",
    "\n",
    "loader_local = UnstructuredLoader(\n",
    "    file_path=file_path,\n",
    "    strategy=\"hi_res\",\n",
    "    languages=[\"spa\"],\n",
    ")\n",
    "\n",
    "try:\n",
    "    documents = loader_local.load()\n",
    "    print(f\"Se extrajeron {len(documents)} fragmentos.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Coloca tu PDF en data/docs/xxxxx.pdf o actualiza la ruta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders\n",
    "Los `DocumentLoader` normalizan fuentes heterogéneas: archivos, sitios web, bases de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "\n",
    "# PDF\n",
    "pdf_loader = PyPDFLoader(\"data/docs/ejemplo.pdf\")\n",
    "\n",
    "# Web\n",
    "web_loader = WebBaseLoader(\"https://docs.langchain.com/oss/python/langchain/overview\")\n",
    "\n",
    "print(\"Loaders listos. Ejecuta load() cuando quieras materializar los documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores (FAISS local)\n",
    "FAISS es un índice vectorial rápido para búsquedas semánticas en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "112ee2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de embeddings 768\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Assuming you have your documents and embeddings initialized\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"ai/granite-embedding-multilingual:latest\",\n",
    "    api_key=\"dummy\",  \n",
    "    base_url=\"http://localhost:12434/engines/llama.cpp/v1\"\n",
    ")\n",
    "\n",
    "print(f'Dimension de embeddings {len(embeddings.embed_query(\"hello world\"))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15514edc",
   "metadata": {},
   "source": [
    "https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2557f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "index = faiss.IndexFlatL2(768)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore= InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "vector_store.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15e238a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added successfully\n"
     ]
    }
   ],
   "source": [
    "# Create documents with valid text content\n",
    "document_1 = Document(page_content=\"LangChain es una herramienta de orquestación\", metadata={\"tipo\": \"descripcion\"})\n",
    "document_2 = Document(page_content=\"Los agentes pueden usar múltiples herramientas\", metadata={\"tipo\": \"funcionalidad\"})\n",
    "document_3 = Document(page_content=\"FAISS permite búsquedas vectoriales eficientes\", metadata={\"tipo\": \"componente\"})\n",
    "\n",
    "documents = [document_1, document_2, document_3]\n",
    "ids = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "\n",
    "try:\n",
    "\tvector_store.add_documents(documents=documents, ids=ids)\n",
    "\tprint(\"Documents added successfully\")\n",
    "except Exception as e:\n",
    "\tprint(f\"Error adding documents: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff9d89e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def clean(text):\n",
    "    import re\n",
    "    text = unidecode(text)  # Convierte tildes a ASCII\n",
    "    return re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "\n",
    "# Aplica clean al query\n",
    "query = \"¿Quién controla el flujo de un agente complejo?\"\n",
    "\n",
    "results = vector_store.similarity_search(query=query, k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vector_store = FAISS.load_local(\n",
    "\"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Próximos pasos\n",
    "1. Conecta LangSmith para trazar y depurar tus cadenas en producción.\n",
    "2. Lleva tus agentes a LangGraph cuando necesiten ciclos o ramificaciones complejas.\n",
    "3. Experimenta con `RunnableWithRetry`, `ToolNode` y `StateGraph` para robustecer tus agentes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
